{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- The predicted rating of user $u$ on item $i$,\n",
    "\n",
    "    $$\\hat{r}_{ui}=b_u+b_i+\\bar{U}_u^{-i}V_i^T$$\n",
    "\n",
    "    where,\n",
    "\n",
    "    $$\\bar{U}_{u\\cdot}^{-i}=\\frac1{|\\mathcal{I}_u\\setminus\\{i\\}|^\\alpha}\\sum_{i^{\\prime}\\in\\mathcal{I}_u\\setminus\\{i\\}}W_{i^{\\prime}\\cdot},\\:0\\leq\\alpha\\leq1$$\n",
    "\n",
    "    Note that when $i\\notin I_u,I_u=I_u\\backslash\\{i\\}.$\n",
    "-  The objective function of FISMrmse,\n",
    "\n",
    " $$\\min_{\\Theta}\\sum_{(u,i)\\in\\mathcal{P}\\cup\\mathcal{A}}f_{ui}$$\n",
    "\n",
    "where $\\Theta=\\{V_i.,W_{i^{\\prime}.},b_u,b_i\\},i,i^{\\prime}=1,\\ldots,m,u=1,\\ldots,n$, and\n",
    "\n",
    "$$f_{ui}=\\frac{1}{2}(r_{ui}-\\hat{r}_{ui})^{2}+\\frac{\\alpha_{v}}{2}||V_{i.}||_{F}^{2}+\\frac{\\alpha_{w}}{2}\\sum_{{i^{\\prime}\\in\\mathcal{I}_{u}\\setminus\\{i\\}}}||W_{i^{\\prime}.}||_{F}^{2}+\\frac{\\beta_{u}}{2}b_{u}^{2}+\\frac{\\beta_{v}}{2}b_{i}^{2}$$\n",
    "\n",
    "- Notes:\n",
    "    - $\\mathcal{P}$ is the whole set of observed (user, item) pairs\n",
    "    - $A$ isa sampled set of unobserved (user, item) pairs\n",
    "    - According to the loss function, we can see that $FISM_{rmse}$ is a pointwise method\n",
    "- 进行梯度下降更新即可，评估指标Pre@5 ， Rec@5:\n",
    "$\\begin{aligned}&&\\text{For each }(u,i)\\in&\\mathcal{P}\\cup\\mathcal{A},\\text{we have the gradients},\\\\&&\\nabla b_{u}=&\\frac{\\partial f_{ui}}{\\partial b_u}=-e_{ui}+\\beta_ub_u\\\\&&\\nabla b_{i}=&\\frac{\\partial f_{ui}}{\\partial b_i}=-e_{ui}+\\beta_vb_i\\\\&&\\nabla V_{i.}=&\\frac{\\partial f_{ui}}{\\partial V_{i.}}=-e_{ui}\\bar{U}_{u.}^{-i}+\\alpha_{v}V_{i.}\\\\&&\\nabla W_{i^{\\prime}}.\\quad=&\\frac{\\partial f_{ui}}{\\partial W_{i^{\\prime}.}}=-e_{ui}\\frac{1}{|\\mathcal{I}_{u}\\backslash\\{i\\}|^{\\alpha}}V_{i.}+\\alpha_{w}W_{i^{\\prime}.},i^{\\prime}\\in\\mathcal{I}_{u}\\backslash\\{i\\}\\\\&&wheree_{ui}=r_{ui}&-\\hat{r}_{ui}.\\mathrm{Note~that}r_{ui}=1\\mathrm{if}(u,i)\\in\\mathcal{P},\\mathrm{and}r_{ui}=0\\mathrm{if}\\\\&(u,i)\\in\\mathcal{A}.\\end{aligned}$\n",
    "- $\\begin{gathered}\\text{For each }(u,i)\\in\\mathcal{P}\\cup\\mathcal{A},\\text{ we have the update rules,}\\\\b_u=\\quad b_u-\\gamma\\nabla b_u\\\\b_i=\\quad b_i-\\gamma\\nabla b_i\\\\V_{i.}=\\quad V_{i.}-\\gamma\\nabla V_i.\\\\W_{i^{\\prime}.}=\\quad W_{i^{\\prime}.}-\\gamma\\nabla W_{i^{\\prime}.},i^{\\prime}\\in\\mathcal{I}_u\\backslash\\{i\\}\\\\\\mathrm{where~}e_{ui}=r_{ui}-\\hat{r}_{ui}.\\text{ Note that }r_{ui}=1\\mathrm{~if~}(u,i)\\in\\mathcal{P},\\mathrm{~and~}r_{ui}=0\\mathrm{~if}\\\\(u,i)\\in\\mathcal{A}.\\end{gathered}$\n",
    "- 1: Initialize the model parameters $\\Theta$\n",
    "\n",
    "2: for t = 1, . . . , T do\n",
    "\n",
    "3: Randomly pick upa set $\\mathcal{A}$ with $|\\mathcal{A}|=\\rho|\\mathcal{P}|$\n",
    "\n",
    "4: for each $(u,i)\\in\\mathcal{P}\\cup\\mathcal{A}$ in a random order do\n",
    "\n",
    "5:Calculate $\\bar{U}_{u.}^{-i}=\\frac1{|\\mathcal{I}_{u}\\setminus\\{i\\}|^{\\alpha}}\\sum_{i^{\\prime}\\in\\mathcal{I}_{u}\\setminus\\{i\\}}W_{i^{\\prime}.}$\n",
    "\n",
    "6:Calculate $\\hat{r}_ui=b_u+b_i+\\bar{U}_u^{-i}V_{i.}^T$\n",
    "\n",
    "7:Calculate $e_{ui}=r_{ui}-\\hat{r}_{ui}$\n",
    "\n",
    "8:Update the $b_u,b_i,V_i.$ and $W_{i^{\\prime}.},i^{\\prime}\\in\\mathcal{I}_u\\backslash\\{i\\}$\n",
    "\n",
    "9: end for \n",
    "\n",
    "10: end for\n",
    "- We use the files u1.base and u1.test of MovieLens100K as ourtraining data and test data, respectively.\n",
    "\n",
    "user number: n = 943; item number: m = 1682.\n",
    "\n",
    "u1.base (training data): 80000 rating records, and the density (or sparsity) is 80000/943/1682 = 5.04%.\n",
    "- We use the statistics of training data to initialize the model parameters,\n",
    "\n",
    "$$\\begin{aligned}b_{u}&=\\quad\\sum_{i=1}^my_{ui}/m-\\mu\\\\b_{i}&=\\quad\\sum_{u=1}^ny_{ui}/n-\\mu\\\\V_{ik}&=\\quad(r-0.5)\\times0.01,k=1,\\ldots,d\\\\W_{i^{\\prime}k}&=\\quad(r-0.5)\\times0.01,k=1,\\ldots,d\\end{aligned}$$\n",
    "\n",
    "\n",
    "- where r (0 ≤ r < 1) is a random variable, and μ =$\\sum_{u=1}^n\\sum_{i=1}^my_{ui}/n/m.$\n",
    "- $\\begin{aligned}&\\text{We fix }\\alpha=0.5\\mathrm{~and~}\\gamma=0.01,\\text{ and search the best values of the}\\\\&\\text{following parameters,}\\\\&\\bullet\\alpha_v=\\alpha_w=\\beta_u=\\beta_v\\in\\{0.001,0.01,0.1\\}\\\\&\\bullet T\\in\\{100,500,1000\\}\\\\&\\text{Finally, we use }\\alpha=0.5,\\gamma=0.01,\\rho=3,d=20,\\\\&\\alpha_v=\\alpha_w=\\beta_u=\\beta_v=0.001\\mathrm{~and~}T=100,\\text{which performs best in our}\\\\&\\text{experiments.}\\\\&\\text{lt takes about 75 seconds for training.}\\end{aligned}$\n",
    "- FISMrmse:Pre@5:0.3846,Rec@5:0.1270."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:25<00:00, 13.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FISM_rmse:\n",
      "Pre@5:0.3899\n",
      "Rec@5:0.1246\n",
      "Running time:1368.29s\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class FISM_rmse:\n",
    "    def __init__(self, train_data_file, test_data_file, T=100, d=20, learning_rate=0.01, regularization=0.001, alpha=0.5,\n",
    "                 p=3):\n",
    "        # initialize the model parameters\n",
    "        self.p = p\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.user_num = 943\n",
    "        self.item_num = 1682\n",
    "        self.items = set(range(1, self.item_num + 1))\n",
    "        self.bi = np.zeros(self.item_num + 1)\n",
    "        self.bu = np.zeros(self.user_num + 1)\n",
    "        self.user_item_matrix = np.zeros((self.user_num + 1, self.item_num + 1))\n",
    "\n",
    "        # load the data and process it\n",
    "        u_train = pd.read_csv(train_data_file, sep='\\t', header=None,\n",
    "                              names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "        u_train = u_train[u_train['rating'] > 3]\n",
    "        self.observed_records = []\n",
    "        u_test = pd.read_csv(test_data_file, sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "        self.train_user_items = {}\n",
    "        self.train_item_users = {}\n",
    "        count = 0\n",
    "        for index, row in u_train.iterrows():\n",
    "            count += 1\n",
    "            self.user_item_matrix[row['user_id']][row['item_id']] = 1\n",
    "            self.train_item_users.setdefault(row['item_id'], set())\n",
    "            self.train_user_items.setdefault(row['user_id'], set())\n",
    "            self.train_item_users[row['item_id']].add(row['user_id'])\n",
    "            self.train_user_items[row['user_id']].add(row['item_id'])\n",
    "            self.observed_records.append((row['user_id'], row['item_id'], 1))\n",
    "\n",
    "        self.unobserved_records = []\n",
    "        self.test_data_users = set()\n",
    "        self.test_user_items = {}\n",
    "        for index, row in u_test.iterrows():\n",
    "            if row['rating'] > 3:\n",
    "                self.test_user_items.setdefault(row['user_id'], set())\n",
    "                self.test_user_items[row['user_id']].add(row['item_id'])\n",
    "                self.test_data_users.add(row['user_id'])\n",
    "\n",
    "        # compute the bias of each item\n",
    "        miu = count / (self.item_num * self.user_num)\n",
    "        for i in range(1, self.item_num + 1):\n",
    "            self.train_item_users.setdefault(i, set())\n",
    "            self.bi[i] = self.train_item_users[i].__len__() / self.user_num - miu\n",
    "\n",
    "        for i in range(1, self.user_num + 1):\n",
    "            self.train_user_items.setdefault(i, set())\n",
    "            self.bu[i] = self.train_user_items[i].__len__() / self.item_num - miu\n",
    "\n",
    "        for i in range(1, self.user_num + 1):\n",
    "            for j in range(1, self.item_num + 1):\n",
    "                if self.user_item_matrix[i][j] == 0:\n",
    "                    self.unobserved_records.append((i, j, 0))\n",
    "\n",
    "        # initialize the latent matrix\n",
    "        self.V = np.random.rand(self.item_num + 1, self.d)\n",
    "        self.W = np.random.rand(self.item_num + 1, self.d)\n",
    "\n",
    "        self.V = (self.V - 0.5) * 0.01\n",
    "        self.W = (self.W - 0.5) * 0.01\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        U_ = np.zeros(self.d, dtype=float)\n",
    "        diff = self.train_user_items[user_id] - {item_id}\n",
    "        rating_count = len(diff)\n",
    "        if rating_count <= 0:\n",
    "            return self.bu[user_id] + self.bi[item_id], diff, U_\n",
    "        for item in diff:\n",
    "            U_ = U_ + self.W[item]\n",
    "        U_ = U_ / math.pow(rating_count, self.alpha)\n",
    "        return np.dot(U_, self.V[item_id]) + self.bu[user_id] + self.bi[item_id], diff, U_\n",
    "\n",
    "    def train(self):\n",
    "        unobserved_records_length = len(self.unobserved_records)\n",
    "        sample_length = len(self.observed_records) * self.p\n",
    "        observed_records_set = set(self.observed_records)\n",
    "        for t in tqdm(range(self.T)):\n",
    "            all_index = range(unobserved_records_length)\n",
    "            sample_index = random.sample(all_index, sample_length)\n",
    "            sample_set = set()\n",
    "            for i in sample_index:\n",
    "                sample_set.add(self.unobserved_records[i])\n",
    "            total_set = sample_set | observed_records_set\n",
    "            for record in total_set:\n",
    "                user_id = record[0]\n",
    "                item_id = record[1]\n",
    "\n",
    "                r_prediction, diff, U_ = self.predict(user_id, item_id)\n",
    "                eui = record[2] - r_prediction\n",
    "                if len(diff) != 0:\n",
    "                    fm = math.pow(len(diff), self.alpha)\n",
    "                    self.W[list(diff)] -= self.learning_rate * (\n",
    "                            self.regularization * self.W[list(diff)] - (eui / fm) * self.V[item_id])\n",
    "\n",
    "                gradient_V = self.regularization * self.V[item_id] - eui * U_\n",
    "                gradient_bu = self.regularization * self.bu[user_id] - eui\n",
    "                gradient_bi = self.regularization * self.bi[item_id] - eui\n",
    "\n",
    "                self.V[item_id] -= self.learning_rate * gradient_V\n",
    "                self.bu[user_id] -= self.learning_rate * gradient_bu\n",
    "                self.bi[item_id] -= self.learning_rate * gradient_bi\n",
    "\n",
    "    def test(self, recommend_num=5):\n",
    "        Pre_K = 0.0\n",
    "        Rec_K = 0.0\n",
    "        # compute the precision and recall of the model on test data set while the recommendation list length is 5\n",
    "        for user in self.test_data_users:\n",
    "            diff = self.items - self.train_user_items[user]\n",
    "            user_item_rating_prediction = np.zeros(self.item_num + 1)\n",
    "            for item in diff:\n",
    "                user_item_rating_prediction[item], temp, U_ = self.predict(user, item)\n",
    "            diff = set(sorted(diff, key=lambda x: user_item_rating_prediction[x], reverse=True)[0:recommend_num])\n",
    "            Pre_K += len(diff & self.test_user_items.get(user, set())) / recommend_num\n",
    "            Rec_K += len(diff & self.test_user_items.get(user, set())) / len(self.test_user_items.get(user, set()))\n",
    "        Pre_K /= len(self.test_data_users)\n",
    "        Rec_K /= len(self.test_data_users)\n",
    "        print(f\"FISM_rmse:\")\n",
    "        print(f'Pre@{recommend_num}:{Pre_K:.4f}')\n",
    "        print(f'Rec@{recommend_num}:{Rec_K:.4f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    FISM = FISM_rmse(r\"E:\\datasets\\ml-100k\\u1.base\", r\"E:\\datasets\\ml-100k\\u1.test\")\n",
    "    FISM.train()\n",
    "    FISM.test()\n",
    "    end = time.time()\n",
    "    print(f'Running time:{end - start:.2f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
