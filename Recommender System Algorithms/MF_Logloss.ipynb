{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 7/100 [00:18<04:10,  2.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    123\u001b[0m    \u001b[38;5;66;03m# start = time.time()\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     mfLogLoss \u001b[38;5;241m=\u001b[39m MFLogLoss(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mml-100k\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mu1.base\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mml-100k\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mu1.test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m     mfLogLoss\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    126\u001b[0m     mfLogLoss\u001b[38;5;241m.\u001b[39mtest()\n",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m, in \u001b[0;36mMFLogLoss.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m user_id \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     90\u001b[0m item_id \u001b[38;5;241m=\u001b[39m record[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 92\u001b[0m r_prediction\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(user_id, item_id)\n\u001b[0;32m     93\u001b[0m eui \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(record[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(r_prediction\u001b[38;5;241m*\u001b[39mrecord[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m     95\u001b[0m gradient_U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularization \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU[user_id] \u001b[38;5;241m-\u001b[39m eui \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV[item_id]\n",
      "Cell \u001b[1;32mIn[3], line 75\u001b[0m, in \u001b[0;36mMFLogLoss.predict\u001b[1;34m(self, user_id, item_id)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_id, item_id):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbu[user_id] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbi[item_id] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU[user_id], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV[item_id])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MFLogLoss:\n",
    "    def __init__(self, train_data_file, test_data_file, T=100, d=20, learning_rate=0.01, regularization=0.001, alpha=0.5,\n",
    "                 p=3):\n",
    "        # initialize the model parameters\n",
    "        self.p = p\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.user_num = 943\n",
    "        self.item_num = 1682\n",
    "        self.items = set(range(1, self.item_num + 1))\n",
    "        self.bi = np.zeros(self.item_num + 1)\n",
    "        self.bu = np.zeros(self.user_num + 1)\n",
    "        self.user_item_matrix = np.zeros((self.user_num + 1, self.item_num + 1))\n",
    "\n",
    "        # load the data and process it\n",
    "        u_train = pd.read_csv(train_data_file, sep='\\t', header=None,\n",
    "                              names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "        u_train = u_train[u_train['rating'] > 3]\n",
    "        self.observed_records = []\n",
    "        u_test = pd.read_csv(test_data_file, sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "        self.train_user_items = {}\n",
    "        self.train_item_users = {}\n",
    "        count = 0\n",
    "        for index, row in u_train.iterrows():\n",
    "            count += 1\n",
    "            self.user_item_matrix[row['user_id']][row['item_id']] = 1\n",
    "            self.train_item_users.setdefault(row['item_id'], set())\n",
    "            self.train_user_items.setdefault(row['user_id'], set())\n",
    "            self.train_item_users[row['item_id']].add(row['user_id'])\n",
    "            self.train_user_items[row['user_id']].add(row['item_id'])\n",
    "            self.observed_records.append((row['user_id'], row['item_id'], 1))\n",
    "\n",
    "        self.unobserved_records = []\n",
    "        self.test_data_users = set()\n",
    "        self.test_user_items = {}\n",
    "        for index, row in u_test.iterrows():\n",
    "            if row['rating'] > 3:\n",
    "                self.test_user_items.setdefault(row['user_id'], set())\n",
    "                self.test_user_items[row['user_id']].add(row['item_id'])\n",
    "                self.test_data_users.add(row['user_id'])\n",
    "\n",
    "        # compute the bias of each item\n",
    "        miu = count / (self.item_num * self.user_num)\n",
    "        for i in range(1, self.item_num + 1):\n",
    "            self.train_item_users.setdefault(i, set())\n",
    "            self.bi[i] = self.train_item_users[i].__len__() / self.user_num - miu\n",
    "\n",
    "        for i in range(1, self.user_num + 1):\n",
    "            self.train_user_items.setdefault(i, set())\n",
    "            self.bu[i] = self.train_user_items[i].__len__() / self.item_num - miu\n",
    "\n",
    "        for i in range(1, self.user_num + 1):\n",
    "            for j in range(1, self.item_num + 1):\n",
    "                if self.user_item_matrix[i][j] == 0:\n",
    "                    self.unobserved_records.append((i, j, -1))\n",
    "\n",
    "        # initialize the latent matrix\n",
    "        self.V = np.random.rand(self.item_num + 1, self.d)\n",
    "        self.U = np.random.rand(self.user_num + 1, self.d)\n",
    "\n",
    "        self.V = (self.V - 0.5) * 0.01\n",
    "        self.U = (self.U - 0.5) * 0.01\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        return self.bu[user_id] + self.bi[item_id] + np.dot(self.U[user_id], self.V[item_id])\n",
    "\n",
    "    def train(self):\n",
    "        unobserved_records_length = len(self.unobserved_records)\n",
    "        sample_length = len(self.observed_records) * self.p\n",
    "        observed_records_set = set(self.observed_records)\n",
    "        for t in tqdm(range(self.T)):\n",
    "            all_index = range(unobserved_records_length)\n",
    "            sample_index = random.sample(all_index, sample_length)\n",
    "            sample_set = set()\n",
    "            for i in sample_index:\n",
    "                sample_set.add(self.unobserved_records[i])\n",
    "            total_set = sample_set | observed_records_set\n",
    "            for record in total_set:\n",
    "                user_id = record[0]\n",
    "                item_id = record[1]\n",
    "\n",
    "                r_prediction= self.predict(user_id, item_id)\n",
    "                eui = float(record[2]) / (1+np.exp(r_prediction*record[2]))\n",
    "\n",
    "                gradient_U = self.regularization * self.U[user_id] - eui * self.V[item_id]\n",
    "                gradient_V = self.regularization * self.V[item_id] - eui * self.U[user_id]\n",
    "                gradient_bu = self.regularization * self.bu[user_id] - eui\n",
    "                gradient_bi = self.regularization * self.bi[item_id] - eui\n",
    "\n",
    "                self.U[user_id] -= self.learning_rate * gradient_U\n",
    "                self.V[item_id] -= self.learning_rate * gradient_V\n",
    "                self.bu[user_id] -= self.learning_rate * gradient_bu\n",
    "                self.bi[item_id] -= self.learning_rate * gradient_bi\n",
    "    def test(self, recommend_num=5):\n",
    "        Pre_K = 0.0\n",
    "        Rec_K = 0.0\n",
    "        # compute the precision and recall of the model on test data set while the recommendation list length is 5\n",
    "        for user in self.test_data_users:\n",
    "            diff = self.items - self.train_user_items[user]\n",
    "            user_item_rating_prediction = np.zeros(self.item_num + 1)\n",
    "            for item in diff:\n",
    "                user_item_rating_prediction[item]= self.predict(user, item)\n",
    "            diff = set(sorted(diff, key=lambda x: user_item_rating_prediction[x], reverse=True)[0:recommend_num])\n",
    "            Pre_K += len(diff & self.test_user_items.get(user, set())) / recommend_num\n",
    "            Rec_K += len(diff & self.test_user_items.get(user, set())) / len(self.test_user_items.get(user, set()))\n",
    "        Pre_K /= len(self.test_data_users)\n",
    "        Rec_K /= len(self.test_data_users)\n",
    "        print(f\"MFLogloss:\")\n",
    "        print(f'Pre@{recommend_num}:{Pre_K:.4f}')\n",
    "        print(f'Rec@{recommend_num}:{Rec_K:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   # start = time.time()\n",
    "    mfLogLoss = MFLogLoss(r\"E:\\datasets\\ml-100k\\u1.base\", r\"E:\\datasets\\ml-100k\\u1.test\")\n",
    "    mfLogLoss.train()\n",
    "    mfLogLoss.test()\n",
    "   # end = time.time()\n",
    "    #print(f'Running time:{end - start:.2f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
